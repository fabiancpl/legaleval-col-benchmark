{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3283866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafac0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf303073",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_PATH = \"./tasks/{}.jsonl\"\n",
    "PROMPT_TEMPLATE_PATH = \"./templates/{}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5de5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"lawyers-exam\"\n",
    "\n",
    "backend = \"hf\"  # hf, openai, anthropic\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"  # meta-llama/Llama-3.2-3B-Instruct, gpt-5, claude-opus-4-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199d6d7",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark(path):\n",
    "    with open(path) as f:\n",
    "        return [json.loads(l) for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_benchmark(BENCHMARK_PATH.format(task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897999f1",
   "metadata": {},
   "source": [
    "## Load the model / client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76115f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if backend == \"openai\":\n",
    "    client = OpenAI()\n",
    "elif backend == \"anthropic\":\n",
    "    client = anthropic.Anthropic()\n",
    "elif backend == \"hf\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=model.device,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        temperature=0.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96feac",
   "metadata": {},
   "source": [
    "## Load the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71630ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_template():\n",
    "    with open(PROMPT_TEMPLATE_PATH.format(task_name), \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = load_template()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff80a61",
   "metadata": {},
   "source": [
    "## Format prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(situation, question, choices):\n",
    "    return prompt_template.format(\n",
    "        situation, question,\n",
    "        choices[0], choices[1], choices[2], choices[3]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_prompt(dataset[0][\"situacion\"], dataset[0][\"enunciado\"], dataset[0][\"opciones\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09224e6",
   "metadata": {},
   "source": [
    "## Extend the dataset by permuting choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_dataset(dataset, m = 12, seed = 42):\n",
    "    \"\"\"\n",
    "    Create m order-perturbed variants per MCQ item with balanced correct-option positions.\n",
    "\n",
    "    For each original example, this function selects m permutations of [0,1,2,3] such that\n",
    "    the correct option appears equally often in each position (a/b/c/d). With m=12, that\n",
    "    means 3 times in each position.\n",
    "\n",
    "    The output dataset contains len(dataset) * m entries. For each original example, its m\n",
    "    variants are returned contiguously (first the m variants of item 0, then item 1, etc.).\n",
    "\n",
    "    TODO: Make dynamic the name of keys.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = random.Random(seed)\n",
    "    all_perms = list(itertools.permutations(range(4)))\n",
    "    extended_dataset = []\n",
    "    r, _ = divmod(m, 4)\n",
    "\n",
    "    for item in dataset:\n",
    "        # Build 4 groups: for each target position k, permutations where index c lands at k\n",
    "        groups = {k: [] for k in range(4)}\n",
    "        for p in all_perms:\n",
    "            pos_c = p.index(item[\"correcta\"])\n",
    "            groups[pos_c].append(p)\n",
    "        \n",
    "        # Sample r permutations from each group to balance positions\n",
    "        chosen = []\n",
    "        for k in range(4):\n",
    "            chosen.extend(rng.sample(groups[k], r))\n",
    "        \n",
    "        rng.shuffle(chosen)  # optional\n",
    "\n",
    "        # Materialize the m variants for this example\n",
    "        for perm in chosen:\n",
    "            new_options = [item[\"opciones\"][i] for i in perm]\n",
    "            new_correct = perm.index(item[\"correcta\"]) \n",
    "            extended_dataset.append({\n",
    "                \"competencia\": item[\"competencia\"],\n",
    "                \"situacion\": item[\"situacion\"],\n",
    "                \"enunciado\": item[\"enunciado\"],\n",
    "                \"opciones\": new_options,\n",
    "                \"correcta\": new_correct\n",
    "            })\n",
    "        \n",
    "    return extended_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bce9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_dataset = extend_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_dataset[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e551ce",
   "metadata": {},
   "source": [
    "## Run inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    if backend == \"openai\":\n",
    "        return client.responses.create(\n",
    "            model=model_id, input=messages,\n",
    "            # Required for non-reasoning models\n",
    "            # max_output_tokens=16, temperature=0,\n",
    "            # Optional for reasoning models\n",
    "            reasoning={\n",
    "                \"effort\": \"minimal\"\n",
    "            }\n",
    "        ).output_text\n",
    "    elif backend == \"anthropic\":\n",
    "        content = client.messages.create(\n",
    "            model=model_id, messages=messages,\n",
    "            max_tokens=5, temperature=0\n",
    "        ).content\n",
    "\n",
    "        if len(content) == 0:\n",
    "            print(\"Error generating content\")\n",
    "            return \"\"\n",
    "        else:\n",
    "            return content[0].text\n",
    "    elif backend == \"hf\":\n",
    "        return pipe(messages, return_full_text=False)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat(format_prompt(extended_dataset[5][\"situacion\"], extended_dataset[5][\"enunciado\"], extended_dataset[5][\"opciones\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df771d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for item in tqdm(extended_dataset):\n",
    "    # Formatting prompt\n",
    "    prompt = format_prompt(item[\"situacion\"], item[\"enunciado\"], item[\"opciones\"])  # type: ignore\n",
    "\n",
    "    # Calling the model\n",
    "    output = chat(prompt)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for output, correct_idx in zip(outputs, [d[\"correcta\"] for d in extended_dataset]):\n",
    "    correct = \"abcd\"[correct_idx]  # type: ignore\n",
    "    results.append(int(output == correct))\n",
    "\n",
    "results_arr = np.reshape(np.array(results), (-1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba012520",
   "metadata": {},
   "source": [
    "## Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-item mean correctness (order-robust correctness per item)\n",
    "per_item_accuracy = results_arr.mean(axis=1)\n",
    "accuracy = per_item_accuracy.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ee5d1",
   "metadata": {},
   "source": [
    "## Persist the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f086a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, model_id):\n",
    "    with open(f\"./results/{task_name}/{model_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results({\n",
    "    \"accuracy\": accuracy\n",
    "}, model_id.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fae92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legaleval-col-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
